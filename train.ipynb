{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "import torch\r\n",
                "from torchvision import transforms\r\n",
                "torch.manual_seed(17)\r\n",
                "from data import XrayImageDataset, CustomLoader"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "# Parameters\r\n",
                "params = {'batch_size': 1,\r\n",
                "          'shuffle': True,\r\n",
                "          'num_workers': 6}\r\n",
                "\r\n",
                "transform_dict = { 'train': transforms.Compose( [transforms.ToTensor(),]),\r\n",
                "        'test': transforms.Compose([transforms.ToTensor() ])}\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "data_path = r\"C:\\Users\\320088652\\OneDrive - Philips\\Philips\\Artefact Detection\\Experiments\\VAE\\BasicVAE\\data\""
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "training_set = XrayImageDataset(data_path, transform = transform_dict['train'])\r\n",
                "training_generator = CustomLoader(training_set, batch_size=8)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "source": [
                "for batch in training_generator:\r\n",
                "    print(batch.shape)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "(25, 128, 128)\n",
                        "torch.Size([128, 25, 128])\n",
                        "(25, 128, 128)\n",
                        "torch.Size([25, 128, 128])\n",
                        "torch.Size([8, 128, 128])\n",
                        "torch.Size([8, 128, 128])\n",
                        "torch.Size([8, 128, 128])\n",
                        "torch.Size([1, 128, 128])\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.8.0",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.8.0 64-bit ('analysis38_env': venv)"
        },
        "interpreter": {
            "hash": "6cfd8a935a17cb1c29249e22d28de3bbf1daddbc779356124f8ed710af677c6d"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}